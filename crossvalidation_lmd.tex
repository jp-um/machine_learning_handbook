\chapter{Cross-Validation}
\label{ch:cross-validation}

Cross-validation (CV) is an estimation method used on supervised learning algorithms to assess their ability to predict the output of unseen data \cite{kohavi1995study}. Supervised learning algorithms are computational tasks like classification or regression, that learn an input-output function based on a set of samples. Such samples are also known as the labeled training data where each example consists of an input vector and its correct output value. After the training phase, a supervised learning algorithm should be able to use the inferred function in order to map new input unseen instances, known as testing data, to their correct output values \cite{caruana2006empirical}. Therefore, cross-validation, also known as out-of-sample testing, tests the function's ability to generalize to unseen situations \cite{kohavi1995study}.

Cross-validation has two types of approaches, being i) the exhaustive cross validation approach which divides all the original samples in every possible way, forming training and test sets to train and test the model, and ii) the non-exhaustive cross validation approach which does not consider all the possible ways of splitting the original samples \cite{arlot2010survey}. Each of these approaches are further divided into different cross-validation methods, which are explained below.

\noindent\textbf{Exhaustive cross-validation}
\begin{itemize}
\item Leave-p-out (LpO) \newline 
This method takes \(p\) samples from the data set as the test set and keeps the remaining as the training set, as shown in Fig. \ref{fig:leavep}a. This is repeated for every combination of test and training set formed from the original data set and the average error is obtained. Therefore, this method trains and tests the algorithm \(n\choose p\) times when the number of samples in the original data set is \(n\), becoming inapplicable when \(p>1\) \cite{arlot2010survey}.

\begin{figure*}
\centering
\begin{subfigure}
  \centering
  \includegraphics[width=.4\linewidth]{leavep.png}
  \caption{Leave-p-Out}
  \end{subfigure}
 \begin{subfigure}
  \centering
  \includegraphics[width=.37\linewidth]{leave1.png}
  \caption{Leave-One-Out}
  \end{subfigure}
  \caption{Exhaustive cross-validation methods}
  \label{fig:leavep}
\end{figure*}

\item Leave-one-out (LOO)\newline 
This method is a specific case of the LpO method having \(p=1\). It requires less computation efforts than LpO since the process is only repeated \(n \choose 1\) \(= n\) times, however might still be inapplicable for large values of \(n\) \cite{arlot2010survey}. 
\end{itemize}

\noindent\textbf{Non-exhaustive cross-validation}
\begin{itemize}
\item Holdout method \newline
This method randomly splits the original data set into two sets being the training set and the test set. Usually, the test set is smaller than the training set so that the algorithm has more data to train on. This method involves a single run and so must be used carefully to avoid misleading results. It is therefore sometimes not considered a CV method \cite{kohavi1995study}.

\item \(k\)-fold\newline
This method randomly splits the original data set into \(k\) equally sized subsets, as shown in Fig. \ref{fig:kfold}. The function is then trained and validated \(k\) times, each time taking a different subset as the test data and the remaining \((k-1)\) subsets as the training data, using each of the \(k\) subsets as the test set once. The \(k\) results are averaged to produce a single estimation. Stratified k-fold cross validation is a refinement of the k-fold method, which splits the original samples into equally sized and distributed subsets, having the same proportions of the different target labels \cite{kohavi1995study}.

\begin{figure}
\centering
  \includegraphics[width=0.5\linewidth]{kfold.png}
  \caption{k-Fold Cross Validation where k=4}
  \label{fig:kfold}
\end{figure}

\item Repeated random sub-sampling\newline
This method is also known as the Monte Carlo CV. It splits the data set randomly with replacement into training and test subsets using some predefined split percentage, for every run. Therefore, this generates new training and test data for each run but the test data of the different runs might contain repeated samples, unlike that of \(k\)-fold \cite{xu2001monte}.
\end{itemize}

All of the above cross-validation methods are used to check whether the model has been overfitted or underfitted and hence estimating the model's ability of fitting to independent data . Such ability is measured using quantitative metrics appropriate for the model and data \cite[-3\baselineskip]{kohavi1995study, arlot2010survey}. In the case of classification problems, the misclassification error rate is usually used whilst for regression problems, the mean squared error (MSE) is usually used. MSE is represented by Eq. \ref{mse}, where n is the total number of test samples, \(Y_i\) is the true value of the \(i^{th}\) instance and \(\hat{Y}_i\) is the predicted value of the \(i^{th}\) instance.

\begin{equation}\label{mse}
MSE = \frac{1}{n}\sum^{n}_{i=1}(Y_i - \hat{Y}_i)^2
\end{equation}

Underfitting is when the model has a low degree and so is not flexible enough to fit the data making the model have a low variance and high bias \cite{baumann2003cross}, as seen in Fig. \ref{fig:models}a. Variance is the model's dependence on the training data and bias is model's assumption about the shape of the data \cite{arlot2010survey}. On the other hand, as seen in Fig. \ref{fig:models}b, overfitting is when the model has a too high degree causing it to exactly fit the data as well as the noise and so lacks the ability to generalize \cite{baumann2003cross}, making the model have a high variance. Cross-validation helps reduce this bias and variance since it uses most of the data for both fitting and testing and so helps the model learn the actual relationship within the data. This makes cross-validation a good technique for models to acquire a good bias-variance tradeoff \cite{arlot2010survey}.

\begin{figure*}
\centering
\begin{subfigure}
  \centering
  \includegraphics[width=0.4\linewidth]{underfitting.png}
  \caption{Underfitting}
  \label{fig:underfitting}
\end{subfigure}
\begin{subfigure}
  \centering
  \includegraphics[width=.4\linewidth]{overfitting.png}
  \caption{Overfitting}
  \label{fig:overfitting}
\end{subfigure}
\caption{Model Fitting}
\label{fig:models}
\end{figure*}

As stated in \cite{Kohavi1995study}, the LOO method gives a 0\% accuracy on the test set when the number of target labels are equal to the number of instances in the dataset. It is shown that the \(k\)-fold CV method gives much better results, due to its lower variance, especially when \(k = \{10, 20\}\). Furthermore, R. Kohavi et al. state that the best accuracy is achieved when using the stratified cross-validation method, since this has the least bias.

Therefore, lets take an example using the stratified \(k\)-fold cross-validation method with \(k=10\). Let's say that we are trying to solve age group classification, using eight non-overlapping age groups being 0-5, 6-10, 11-20, 21-30, 31-40, 41-50, 51-60, and 61+. We are using the FG-NET labelled data set, which contains around 1000 images of individuals aged between 0 and 69. Before we can start training our model, we must divide our data set into training and test subsets and this is where cross validation comes in. Therefore, we start by taking the 1000 images of our data set and splitting them according to their target class. Let us assume we have an equal amount of 125 \((1000/8)\) images per class\footnote{Down-sampling or up-sampling are common techniques used when there is an unequal amount of samples for the different classes.}. We can now start forming our 10 folds by taking 10\% of each age-group bucket, randomly without replacement. Hence, we will end up with 10 subsets of 100 images that are equally distributed along all age-groups. With these subsets, we can estimate our model's accuracy with a lower bias-variance tradeoff. Since we are using 10-fold CV, we will train and test our model 10 times. For the first iteration, we shall use subset 1 as the validation set and subsets 2 to 10 as the training set, for the second iteration we use subset 2 as the test set and subsets 1 plus 3 to 10 as our training set, and so on (as shown in Fig. \ref{fig:kfold}). For each iteration we use the misclassification error rate to obtain an accuracy value and we finally average the 10 accuracy rates to obtain the global accuracy of our model when solving age group classification, given the FG-NET data set.

\index{class options|)}
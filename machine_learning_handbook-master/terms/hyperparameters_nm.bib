% Encoding: UTF-8

@Unknown{SathiyaKeerthi2006,
  author  = {Sathiya Keerthi, S and Sindhwani, Vikas and Chapelle, Olivier},
  title   = {An Efficient Method for Gradient-Based Adaptation of Hyperparameters in SVM Models.},
  month   = {01},
  year    = {2006},
  journal = {Advances in Neural Information Processing Systems 19: Proceedings of the 2006 Conference, 673-680 (2007)},
  pages   = {673-680},
  volume  = {19},
}

@Article{MacKay1999,
  author   = {MacKay, David J. C.},
  title    = {Comparison of Approximate Methods for Handling Hyperparameters},
  journal  = {Neural Computation},
  year     = {1999},
  volume   = {11},
  number   = {5},
  pages    = {1035-1068},
  abstract = { I examine two approximate methods for computational implementation of Bayesian hierarchical models, that is, models that include unknown hyperparameters such as regularization constants and noise levels. In the evidence framework, the model parameters are integrated over, and the resulting evidence is maximized over the hyperparameters. The optimized hyperparameters are used to define a gaussian approximation to the posterior distribution. In the alternative MAP method, the true posterior probability is found by integrating over the hyperparameters. The true posterior is then maximized over the model parameters, and a gaussian approximation is made. The similarities of the two approaches and their relative merits are discussed, and comparisons are made with the ideal hierarchical Bayesian solution. In moderately ill-posed problems, integration over hyperparameters yields a probability distribution with a skew peak, which causes signifi-cant biases to arise in the MAP method. In contrast, the evidence framework is shown to introduce negligible predictive error under straightforward conditions. General lessons are drawn concerning inference in many dimensions. },
  doi      = {10.1162/089976699300016331},
  eprint   = {https://doi.org/10.1162/089976699300016331},
  url      = { 
        https://doi.org/10.1162/089976699300016331
    
},
}

@InBook{MacKay1996,
  pages     = {43--59},
  title     = {Hyperparameters: Optimize, or Integrate Out?},
  publisher = {Springer Netherlands},
  year      = {1996},
  author    = {MacKay, David J. C.},
  editor    = {Heidbreder, Glenn R.},
  address   = {Dordrecht},
  isbn      = {978-94-015-8729-7},
  abstract  = {I examine two approximate methods for computational implementation of Bayesian hierarchical models, that is, models which include unknown hyperparameters such as regularization constants. In the `evidence framework' the model parameters are integrated over, and the resulting evidence is maximized over the hyperparameters. The optimized hyperparameters are used to define a Gaussian approximation to the posterior distribution. In the alternative `MAP' method, the true posterior probability is found by integrating over the hyperparameters. The true posterior is then maximized over the model parameters, and a Gaussian approximation is made. The similarities of the two approaches, and their relative merits, are discussed, and comparisons are made with the ideal hierarchical Bayesian solution.},
  booktitle = {Maximum Entropy and Bayesian Methods: Santa Barbara, California, U.S.A., 1993},
  doi       = {10.1007/978-94-015-8729-7_2},
  url       = {https://doi.org/10.1007/978-94-015-8729-7_2},
}

@Article{Cawley2010,
  author  = {Cawley, Gavin and L. C. Talbot, Nicola},
  title   = {On Over-fitting in Model Selection and Subsequent Selection Bias in Performance Evaluation},
  journal = {Journal of Machine Learning Research},
  year    = {2010},
  volume  = {11},
  pages   = {2079-2107},
  month   = {07},
}

@InProceedings{Ito2003,
  author    = {K. Ito and R. Nakano},
  title     = {Optimizing Support Vector regression hyperparameters based on cross-validation},
  booktitle = {Proceedings of the International Joint Conference on Neural Networks, 2003.},
  year      = {2003},
  volume    = {3},
  pages     = {2077-2082 vol.3},
  month     = {July},
  doi       = {10.1109/IJCNN.2003.1223728},
  issn      = {1098-7576},
  keywords  = {optimisation;support vector machines;learning (artificial intelligence);regression analysis;support vector regression;hyper parameters optimisation;penalty factor;kernel function parameter;minimum cross validation;training;coordinate descent method;Optimization methods;Kernel;Performance loss;Neural networks;Indium tin oxide;Paper technology;Pattern recognition;Training data;Smoothing methods;Vectors},
}

@Article{Keerthi2002,
  author   = {S. S. Keerthi},
  title    = {Efficient tuning of SVM hyperparameters using radius/margin bound and iterative algorithms},
  journal  = {IEEE Transactions on Neural Networks},
  year     = {2002},
  volume   = {13},
  number   = {5},
  pages    = {1225-1229},
  month    = {Sept},
  issn     = {1045-9227},
  doi      = {10.1109/TNN.2002.1031955},
  keywords = {learning automata;iterative methods;pattern classification;minimisation;data analysis;SVM hyperparameter tuning;radius/margin bound;iterative algorithms;support vector machine;L/sub 2/ soft margin;iterative techniques;support vectors;Support vector machines;Iterative algorithms;Kernel;Support vector machine classification;Quadratic programming;Polynomials;Mechanical engineering;Algorithm design and analysis;Large-scale systems},
}

@Article{Duan2003,
  author   = {Kaibo Duan and S.Sathiya Keerthi and Aun Neow Poo},
  title    = {Evaluation of simple performance measures for tuning SVM hyperparameters},
  journal  = {Neurocomputing},
  year     = {2003},
  volume   = {51},
  pages    = {41 - 59},
  issn     = {0925-2312},
  abstract = {Choosing optimal hyperparameter values for support vector machines is an important step in SVM design. This is usually done by minimizing either an estimate of generalization error or some other related performance measure. In this paper, we empirically study the usefulness of several simple performance measures that are inexpensive to compute (in the sense that they do not require expensive matrix operations involving the kernel matrix). The results point out which of these measures are adequate functionals for tuning SVM hyperparameters. For SVMs with L1 soft-margin formulation, none of the simple measures yields a performance uniformly as good as k-fold cross validation; Joachimsâ€™ Xi-Alpha bound and the GACV of Wahba et al. come next and perform reasonably well. For SVMs with L2 soft-margin formulation, the radius margin bound gives a very good prediction of optimal hyperparameter values.},
  doi      = {https://doi.org/10.1016/S0925-2312(02)00601-X},
  keywords = {SVM, Model selection, Generalization error bound},
  url      = {http://www.sciencedirect.com/science/article/pii/S092523120200601X},
}

@InProceedings{Thornton2013,
  author       = {Thornton, Chris and Hutter, Frank and Hoos, Holger H and Leyton-Brown, Kevin},
  title        = {Auto-WEKA: Combined selection and hyperparameter optimization of classification algorithms},
  booktitle    = {Proceedings of the 19th ACM SIGKDD international conference on Knowledge discovery and data mining},
  year         = {2013},
  pages        = {847--855},
  organization = {ACM},
}

@Article{Bergstra2013,
  author    = {Bergstra, James and Yamins, Daniel and Cox, David Daniel},
  title     = {Making a science of model search: Hyperparameter optimization in hundreds of dimensions for vision architectures},
  year      = {2013},
  publisher = {JMLR},
}

@Article{Kotthoff2017,
  author    = {Kotthoff, Lars and Thornton, Chris and Hoos, Holger H and Hutter, Frank and Leyton-Brown, Kevin},
  title     = {Auto-WEKA 2.0: Automatic model selection and hyperparameter optimization in WEKA},
  journal   = {The Journal of Machine Learning Research},
  year      = {2017},
  volume    = {18},
  number    = {1},
  pages     = {826--830},
  publisher = {JMLR. org},
}

@Article{Li2017,
  author    = {Li, Lisha and Jamieson, Kevin and DeSalvo, Giulia and Rostamizadeh, Afshin and Talwalkar, Ameet},
  title     = {Hyperband: A novel bandit-based approach to hyperparameter optimization},
  journal   = {The Journal of Machine Learning Research},
  year      = {2017},
  volume    = {18},
  number    = {1},
  pages     = {6765--6816},
  publisher = {JMLR. org},
}

@Article{Bergstra2015,
  author    = {Bergstra, James and Komer, Brent and Eliasmith, Chris and Yamins, Dan and Cox, David D},
  title     = {Hyperopt: a python library for model selection and hyperparameter optimization},
  journal   = {Computational Science \& Discovery},
  year      = {2015},
  volume    = {8},
  number    = {1},
  pages     = {014008},
  publisher = {IOP Publishing},
}

@Article{Kim2014,
  author  = {Kim, Yoon},
  title   = {Convolutional neural networks for sentence classification},
  journal = {arXiv preprint arXiv:1408.5882},
  year    = {2014},
}

@InProceedings{Cox2011,
  author        = {D. Cox and N. Pinto},
  title         = {Beyond simple features A large-scale feature search approach to unconstrained face recognition},
  booktitle     = {Face and Gesture 2011},
  year          = {2011},
  pages         = {8-15},
  month         = {March},
  __markedentry = {[neilm:]},
  doi           = {10.1109FG.2011.5771385},
  keywords      = {computer vision;face recognition;feature extraction;image representation;learning (artificial intelligence);large scale feature search;face recognition;computer vision algorithm;low level feature operator;raw pixel value;subsequent processing;subsequent classification;feature representation;multilayer neuromorphic feature representation;brute-force search;machine learning blending technique;large scale search derived feature set;Kernel;Biological system modeling;Visualization;Face;Brain modeling;Face recognition},
}

@InProceedings{Wang2009,
  author        = {Wang, Yizao and Audibert, Jean-Yves and Munos, R{\'e}mi},
  title         = {Algorithms for infinitely many-armed bandits},
  booktitle     = {Advances in Neural Information Processing Systems},
  year          = {2009},
  pages         = {1729--1736},
  __markedentry = {[neilm:6]},
}

@Comment{jabref-meta: databaseType:bibtex;}

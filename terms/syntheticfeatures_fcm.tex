\chapter{Synthetic Features}
\label{ch:synthetic features}\index{synthetic-features}

\section{The problem}
Training a machine learning algorithm requires inputting some form of training data. This training data comprises of all the features from which the algorithm learns from and builds a model. This input is often referred to as the training dataset.

Whilst in concept the above seems straightforward, it often transpires that the various data-points provided in the training dataset do not fit a structure that is easily understood by the algorithm. For this reason, an important pre-processing step is needed to:

\begin{enumerate}
    \item Understand the original data well
    \item Subsequently, if and where needed, generate synthetic features
\end{enumerate}

If we look at the following example \citep{AlbertoTubeSpam}: It contains a number of records used to train a spam / ham classifier for comments on a YouTube video.

\begin{table}[ht]
    \centering
    \fontfamily{ppl}\selectfont
    \resizebox{\textwidth}{!}{\begin{tabular}{llllll}
      \toprule
      \textit{Video} & \textit{Comment ID} & \textit{Author} & \textit{Date} & \textit{Content} & \textit{Class} \\
      \midrule
      \textit{Psy} & \textit{LZQPQhLyRh9MSZYnf8djyk0gEF9BHDPYrrK-qCczIY8} & \textit{Evgeny Murashkin} & \textit{2013-11-08T17:34:21} & \textit{just for test I have to say murdev.com} & \textit{Spam}  \\
      \textit{Psy} & \textit{z13bgdvyluihfv11i22rgxwhuvabzz1os04} & \textit{Zielimeek21} & \textit{2013-11-28T21:49:00} & \textit{I'm only checking the views} & \textit{Ham}  \\
      \textit{Psy} & \textit{z13kxpqqssa0hlryd04cc1dxeyyngljjngk} & \textit{Tasha Lucius} & \textit{2014-01-19T13:25:56} & \textit{2 billion....Coming soon} & \textit{Ham}  \\
      \textit{Psy} & \textit{z12lg1vizrmsgxm3q23oij4aqrjxjdd1p} & \textit{Holly} & \textit{2014-11-06T13:41:30} & \textit{Follow me on Twitter @mscalifornia95} & \textit{Spam}  \\
      \bottomrule
    \end{tabular}}
    \caption{Sample of four rows from the Psy dataset from the YouTube comment training dataset.}
    \label{tab:sf_origdatasample}
\end{table}
\vspace{2mm}

Table \ref{tab:sf_origdataexplained} describes each feature in the original unmodified dataset.

\begin{table}[ht]
    \centering
    \fontfamily{ppl}\selectfont
    \resizebox{\textwidth}{!}{\begin{tabular}{ll}
      \toprule
      \textit{Feature} & \textit{Description} \\
      \midrule
      \textit{\makecell[tl]{Video}} & \textit{\makecell[tl]{The video this comment was written for. The relevance depends whether the \\ classification model is being built generically for all videos, or a per-video \\ specific model is also considered.}} \\
      \textit{\makecell[tl]{Comment ID}} & \textit{\makecell[tl]{Random comment ID generated by the YouTube comment board system. \\ This probably has no impact on the final class.}} \\
      \textit{\makecell[tl]{Author}} & \textit{\makecell[tl]{The author / account that generated the comment. This has relevance only if \\ this account has a lot of spam comments. \\ If that is the case, two things should happen, none of which are directly \\ related to the machine learning algorithm: \\ \quad - Maintain a blacklist of accounts that are probable spam (if a particular \\ \quad \enspace author often has flagged comments). \\ \quad - Block such accounts.}} \\
      \textit{\makecell[tl]{Date}} & \textit{\makecell[tl]{The date does not directly have a huge relevance on the classification of a \\ comment.}} \\
      \textit{\makecell[tl]{Content}} & \textit{\makecell[tl]{The comment body definitely has a big relevance in the classification result, \\ however, can the whole sentence be easily understood by the algorithm as \\ it is?}} \\
      \bottomrule
    \end{tabular}}
    \caption{Description of each feature in the original unmodified YouTube comment dataset.}
    \label{tab:sf_origdataexplained}
\end{table}
\vspace{2mm}

As one can see, there is very little input the machine learning algorithm can reliably take just from using the four features described above. One could easily realise this by asking oneself the following question (in plain English):

\emph{How can I describe the components of a comment well enough to decide whether it is probably spam or ham?}

One can therefore summarise this problem paradoxically as: \emph{Having enough data to solve the problem, but very little meta-data to actually understand it and solve it.}

\section{Ways of solving the problem}
A way of solving this problem is to apply a synthetic features approach, sometimes referred to as feature engineering. This is the generation of features derived from other existing features, in a way that can be more easily captured or understood by a machine learning algorithm \citep{LiFeatureEng}. It is a way of generating meta-data for the existing features in the original dataset.

In essence, the idea is to look at every available feature and for each determine the following:

\begin{itemize}
    \item Does the feature contain more than one feature within it? If so, try exploding it into sub-features and test.
    \item Does the feature contain too little information for any relevance, but could benefit from adding some context to it? If so, attempt at looking at other features that might be related, and produce new features as a result, and test.
    \item For each of the above, the original feature(s) might not be relevant anymore and be entirely replaced by the newly generated synthetic features instead.
\end{itemize}

What or how an explosion of features or a composite of features is generated depends on the very specific nature of the components involved and there is no generic formula behind it that works without some additional specificness. For example, two pairs of geo-coordinates probably qualify in giving a distance feature, however the formula applied here is specific to the geo-coordinates domain.

There is not a one-size-fits-all approach but rather it is more of an iterative approach with new synthetic features being outputted per iteration, following which one then assesses whether it is enough to generate a reliable machine learning model from the new features or not.

Following below is a practical example of this technique, using the dataset described at the introduction of this chapter.

\subsection{Analysing each feature} \index{synthetic-features!solving-problem-analysing-each-feature}

\begin{itemize}
    \item Video
    \begin{itemize}
        \item The video name / ID could be useful if a per-video classifier is also generated over and above the generic one. This together with other features could have some relevance.
    \end{itemize}
    \item Comment ID
    \begin{itemize}
        \item This feature does not have any relevance to the outcome whatsoever. It is a unique ID, built randomly, assigned to each comment. For this reason, it is out of scope for this discussion.
    \end{itemize}
    \item Author and Date
    \begin{itemize}
        \item As described earlier these two features independently do not have much of a direct impact on the outcome, however a synthetic feature could be generated which might have some form of effect on the outcome: A ratio of comment count over a time period for a particular author.
        The idea is to make it easier for the algorithm to detect a potential pattern related to volume over a typical short period, thus the definition of time period can be assigned via testing.
    \end{itemize}
    \item Comment
    \begin{itemize}
    \item The comment body is not an easy feature and it could grow into a number of features, however it is the most relevant input for this spam classifier.
    Quite a number of features could be exported from this comment, and most of them relate to natural language processing techniques \citep{CormackSMSFilter}. For this reason, output quality could also vary based on the language in context.
    Some example features that could be extrapolated:
    \end{itemize}
\end{itemize}

\begin{table}[ht]
    \centering
    \fontfamily{ppl}\selectfont
    \resizebox{\textwidth}{!}{\begin{tabular}{ll}
      \toprule
      \textit{Synthetic Feature} & \textit{Scope / Description} \\
      \midrule
      \textit{\makecell[tl]{Language}} & \textit{\makecell[tl]{This depends on the availabilities of various NLP implementations for \\ different languages, however one could have an indication of spam / \\ non-spam probabilities based on the comment languages for each \\ particular video.}} \\
      \textit{\makecell[tl]{Readability \\ score}} & \textit{\makecell[tl]{A readability score could be calculated per comment which gives an \\ indication on the quality of such text. An example of such a score \\ could be the \emph{Flesch Reading Ease} score.}} \\
      \textit{\makecell[tl]{Length \\ (excl. stop words)}} & \textit{\makecell[tl]{Very short or very long comments might have a probabilistic impact \\ on the outcome.}} \\
      \textit{\makecell[tl]{Presence of account \\ tags / URLs / emojis}} & \textit{\makecell[tl]{The presence of account tags (ex. a Twitter username), URLs or emojis \\ could increase probability of the comment being spam.}} \\
      \bottomrule
    \end{tabular}}
    \caption{Example of possible features that can be extracted from textual comments.}
    \label{tab:sf_commentextractedfeatures}
\end{table}
\vspace{2mm}

\subsection{Updated feature / data set} \index{synthetic-features!updated-feature-dataset}

Following the synthetic feature generation described above, the updated data set used as an example here would look as follows:

\begin{table}[ht]
    \centering
    \fontfamily{ppl}\selectfont
    \resizebox{\textwidth}{!}{\begin{tabular}{lllllllll}
      \toprule
      \textit{\makecell{Video}} & \textit{\makecell{Author Comments \\ in last minute}} & \textit{\makecell{Language}} & \textit{\makecell{Readability}} & \textit{\makecell{Length excl. \\ stop words}} & \textit{\makecell{Presence of \\ account tags}} & \textit{\makecell{Presence of \\ URLs}} & \textit{\makecell{Presence of \\ emojis}} & \textit{\makecell{Class}} \\
      \midrule
      \textit{Psy} & \textit{1} & \textit{EN} & \textit{94.3} & \textit{3} & \textit{No} & \textit{Yes} & \textit{No} & \textit{Spam}  \\
      \textit{Psy} & \textit{1} & \textit{EN} & \textit{103} & \textit{3} & \textit{No} & \textit{No} & \textit{No} & \textit{Ham}  \\
      \textit{Psy} & \textit{1} & \textit{EN} & \textit{83.3} & \textit{3} & \textit{No} & \textit{No} & \textit{No} & \textit{Ham}  \\
      \textit{Psy} & \textit{1} & \textit{EN} & \textit{32.6} & \textit{3} & \textit{Yes} & \textit{No} & \textit{No} & \textit{Spam}  \\
      \bottomrule
    \end{tabular}}
    \caption{Updated sample of the four rows from the Psy dataset from the YouTube comment training dataset now containg the synthetic features.}
    \label{tab:sf_updatedds}
\end{table}
\vspace{2mm}

Looking at the output in table \ref{tab:sf_updatedds}, the effect of synthetic features can immediately be appreciated, as with such new features more meaning is given to the original dataset.

Naturally the above contains just a sample, and one must experiment with:

\begin{itemize}
    \item more or less synthetic features
    \item a further iteration of synthetic features from the generated features
    \item a much bigger data-set (the example above is too small to build a reliable classifier)
    \item perform feature selection (such as Principle Component Analysis) to identify the features that actually matter and remove extra noise
\end{itemize}

Therefore, employing a synthetic feature approach on your dataset as a pre-processing step, should in general give you positive results.
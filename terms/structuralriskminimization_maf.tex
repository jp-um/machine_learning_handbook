\chapter{Structural Risk Minimization}
\label{ch:structuralriskminimization}\index{structuralriskminimization|(}


Structural Risk Minimization (SRM) is a technique first developed in 1974 by \citet{vapnikchervonenkis1}, used to identify the function $f(x)$ that solves a machine learning problem $y=f(x)$. SRM attempts to find a balance between training the model as accurately as possible, while not having an overly complex solution.

Consider the equation\citep{srmvapnik512} below:

\begin{equation}\label{srm1}
\centering
\min_{f}{\left[\hat{R}(f)+\epsilon(f)\right]}
\end{equation}

The first part of this equation is the training loss, or what we can refer to as the expected risk $\hat{R}(f)$. The expected risk is found based on the training data, and represents the frequency of errors on our training set. Parameters are adjusted to minimize $\hat{R}(f)$ so as to obtain the best possible model for our problem. This technique is referred to as Empirical Risk Minimization (ERM). The second part of the equation $\epsilon(f)$ refers to the capacity measure of the function. This is used to quantify the complexity of our function $f$.

\section {VC-Dimension} \label{sec:vc-dimension}\index{vc-dimension}

A vital topic for one to be able to understand SRM is the VC-Dimension. The theory about uniform convergence of empirical risk to actual risk describes various conditions, as well as the bounds for the rate of convergence. These bounds are based upon a measure of the capacity of the set of functions implemented by the model, also known as the VC-Dimension \citep{srmvapnik506}. Before looking at the definition of the VC-Dimension, it is important to understand the following definition:

\newtheorem{definition}{Definition}
\begin{definition} \label{shatterdefn}
Given a collection $F$ of subsets of a set $S$, we say that the finite subset $A$ of $S$ is shattered provided that every subset $B$ contained in $A$ can be written as intersection of $A$ with an element of $F$. 
\end {definition}
The VC-Dimension is defined\citep{srmvapnik506} as:
\begin{definition} \label{vcdimdefn}
The VC-dimension of a set of indicator functions is the maximum number $h$ of vectors that can be shattered in $2^{h}$ ways by using functions within that set. 
\end {definition}
It should be noted that a VC-Dimension of $h$ does not mean that all sets of $h$ points can be shattered by a given set of functions, but there is at least one example of it occurring.


\section {Structural Risk Minimization} \label{sec:srm} \index{structuralriskminimization}

Suppose $l$ is the size of our sample, and $h$ is our VC dimension. When $\frac{l}{h}$ is large enough, our VC confidence is negligible and hence, it can be ignored and ERM can take place. However, when $\frac{l}{h}$ is small, the VC confidence can no longer be ignored. When this is the case, we need to be able adjust the VC-dimension $h$ of our model \citep{srmvapnik506}. This is where Structural Risk Minimization comes in. It is a technique that allows a trade off in accurate model fitting and avoiding overcomplexity. One may think that we would want our VC-dimension $h$ to be a high value, as our risk $R(f)$ drops. However, while this is true, this would increase the chances of overfitting. This is why we have our complexity term $\epsilon$, as it increases when this occurs. On the other hand, if $h$ is too low, then we would have an issue of our $R(f)$ being too high. So the user must have the right balance of accuracy and complexity.


In mathematical terms, SRM is carried out by first selecting a family of classifiers $\{F(x,w)\}$, and then defining a structure of nested subsets of the elements of the family: $S_{1}\subset S_{2}\subset\ldots\subset S_{n-1}\subset S_{n}$, where $S_{i}=\left\{ f(x,w),\:w\in W_{i}\right\} $. Due to the structure of the subsets, their respective VC-dimension satisfies: $h_{1}<h_{2}<\ldots<h_{n-1}<h_{n}$. Our goal is to find the optimal structure for our problem, denoted $S^{*}$.


To find this optimal structure, we carry out the following steps, as described by \citet{srmvapnik506} and \citet{srmsewell}:

\begin{enumerate}
\item Find the ERM for each element of the structure $\hat{f_{i}}=\mathrm{arg\,\underset{\mathnormal{f}\in \mathnormal{S_{i}}}{min}}\hat{R}(f)$
\item Next we calculate $i_{*}={\mathrm{{arg\,\underset{\mathnormal{i}\in\mathbb{N}}{min}}}}R(\hat{f_{i}})+\epsilon_{i}$
and return $f_{i_{*}}$
\end{enumerate}


\citet{shawetaylorsrm} describes the following theorem, which provides the mathematical framework for the SRM procedure:

\newtheorem{srmtheorem}{Theorem}
\begin{srmtheorem} \label{srmthm1}
Let $S_{1},S_{2},\ldots$ be a sequence of hypothesis classes mapping {X} to $\left(0,1\right)$ and having VC-dimension $i$. Let $\mu$ be any probability measure on $S=X\times\left(0,1\right)$, and let $p_d$, $q_dk$ be any sets of positive numbers satisfying 
\begin{equation}\label{srmthm}
\displaystyle \sum_{d=1}^{\infty} p_{d} = 1
\end{equation}
and $\sum_{k=0}^{m} q_{dk} = 1$ for all $d$. Then with probability $1-\delta$ over $m$ independent identically distributed examples $x$, if the learner finds a hypothesis $f$ in $S_d$ with $Er_{x}(f)=k$, then the generalization error of $f$ is bounded from above by:
\begin{equation}\label{srmthm}
\epsilon(m,d,k,\delta)=\frac{1}{m}\left(2k+4\ln\left(\frac{4}{{p_d}{q_dk}{\delta}}\right)+4d\ln\left(\frac{2em}{d}\right)\right)
\end{equation}
provided $d \leq m$.
\end{srmtheorem}

The function $\epsilon(m,d,k,\delta)$ provides an upper bound on the generalization error of $f$ with confidence $1-\delta$. The above theorem also takes into consideration the possibility of errors occurring within the training set. 

Figure~\ref{fig:srm} is a diagram illustrating the general idea of SRM:

\begin{figure}[h]
\centering
\includegraphics[scale=0.4]{graphics/structuralriskminimization/SRMDiagram.png}
\caption{An illustration of the empirical risk and confidence for different functions. The optimal function is found at the minimum of the bound, listed as $f^{*}$}
\label{fig:srm}
\end{figure}

\section{Examples}\label{sec:examples}\index{examples}


Consider a simple machine learning problem, where we need to find the ideal function to represent our model. As mentioned above, to carry out SRM we have two parts to our procedure, the first of which is finding the function $f_{i}$ that returns to us the minimum risk $R(f)$ for each structure $S_{i}$. The definition of our risk function depends on the type of problem we are solving. For a classification problem, one may want to use a zero-one loss function or a hinge-loss function. On the other hand, for a regression problem, a Huber loss function may be favourable. Once we have selected a suitable loss function, our next step is to look at functions to represent our capacity measure. The $l_{1}$-regularizer and $l_{2}$-regularizer are two examples of functions that can be used to calculate the capacity measure. 


Below is the formula that needs to be solved for a support vector machine, according to SRM:

\begin{equation}\label{srm2}
\centering
\underset{f}{\mathrm{min}}\left[C\cdot\sum_{i=1}^{n}{\mathrm{max}}\left[\left|y_{i}-f(x_{i})\right|,0\right]+\left\Vert \mathbf{w}\right\Vert_{2}^{2}\right]
\end{equation}

with the solution $\hat{f}(x_{i})=C_{k}\cdot K(x,x_{k})$, where $K$ represents the kernel function. Notice how SVMs use the hinge-loss function as their risk function and the $l_{2}$-regularizer to quantify complexity. 

If we are to carry this out for linear regression, our formula would
look something like this:

\begin{equation}\label{srm3}
\centering
\underset{f}{\mathrm{min}}\left[\sum_{i=1}^{n}\left(f(x_{i})-y_{i}\right)^{2}+\left\Vert \mathbf{w}\right\Vert _{2}^{2}\right]
\end{equation}

In reality, we can use whichever regularizer we prefer, however, the $l_{2}$-regularizer has an advantage over the $l_{1}$-regularizer as it is strictly convex and differentiable everywhere.

\index{structuralriskminimization|)}

@misc{pagels_what_2018,
	title = {What is online machine learning?},
	url = {https://medium.com/value-stream-design/online-machine-learning-515556ff72c5},
	abstract = {Making machines learn in real time},
	urldate = {2018-12-21},
	journal = {Value Stream Design},
	author = {Pagels, Max},
	month = apr,
	year = {2018}
}

@article{hoi_libol:_2014,
	title = {{LIBOL}: {A} {Library} for {Online} {Learning} {Algorithms}},
	volume = {15},
	issn = {1532-4435},
	shorttitle = {{LIBOL}},
	url = {http://dl.acm.org/citation.cfm?id=2627435.2627450},
	abstract = {LIBOL is an open-source library for large-scale online learning, which consists of a large family of efficient and scalable state-of-the-art online learning algorithms for large-scale online classification tasks. We have offered easy-to-use command-line tools and examples for users and developers, and also have made comprehensive documents available for both beginners and advanced users. LIBOL is not only a machine learning toolbox, but also a comprehensive experimental platform for conducting online learning research.},
	number = {1},
	urldate = {2018-12-22},
	journal = {J. Mach. Learn. Res.},
	author = {Hoi, Steven C. H. and Wang, Jialei and Zhao, Peilin},
	month = jan,
	year = {2014},
	keywords = {big data analytics, massive-scale classification, online learning},
	pages = {495--499}
}

@INPROCEEDINGS{oza_online_2005, 
author={N. C. Oza}, 
booktitle={2005 IEEE International Conference on Systems, Man and Cybernetics}, 
title={Online bagging and boosting}, 
year={2005}, 
volume={3}, 
number={}, 
pages={2340-2345 Vol. 3}, 
keywords={learning (artificial intelligence);online bagging learning method;online boosting learning method;batch mode;training data;Bagging;Boosting;Backpropagation algorithms;Predictive models;Intelligent systems;NASA;Postal services;Learning systems;Training data;Supervised learning;Bagging;boosting;ensemble learning;online learning}, 
doi={10.1109/ICSMC.2005.1571498}, 
ISSN={1062-922X}, 
month={Oct},}

@article{hoi_online_2018,
	title = {Online {Learning}: {A} {Comprehensive} {Survey}},
	shorttitle = {Online {Learning}},
	url = {http://arxiv.org/abs/1802.02871},
	abstract = {Online learning represents an important family of machine learning algorithms, in which a learner attempts to resolve an online prediction (or any type of decision-making) task by learning a model/hypothesis from a sequence of data instances one at a time. The goal of online learning is to ensure that the online learner would make a sequence of accurate predictions (or correct decisions) given the knowledge of correct answers to previous prediction or learning tasks and possibly additional information. This is in contrast to many traditional batch learning or offline machine learning algorithms that are often designed to train a model in batch from a given collection of training data instances. This survey aims to provide a comprehensive survey of the online machine learning literatures through a systematic review of basic ideas and key principles and a proper categorization of different algorithms and techniques. Generally speaking, according to the learning type and the forms of feedback information, the existing online learning works can be classified into three major categories: (i) supervised online learning where full feedback information is always available, (ii) online learning with limited feedback, and (iii) unsupervised online learning where there is no feedback available. Due to space limitation, the survey will be mainly focused on the first category, but also briefly cover some basics of the other two categories. Finally, we also discuss some open issues and attempt to shed light on potential future research directions in this field.},
	urldate = {2018-12-23},
	journal = {arXiv:1802.02871 [cs]},
	author = {Hoi, Steven C. H. and Sahoo, Doyen and Lu, Jing and Zhao, Peilin},
	month = feb,
	year = {2018},
	note = {arXiv: 1802.02871},
	keywords = {Computer Science - Machine Learning}
}

@inproceedings{zhang_projection-free_2017,
	address = {International Convention Centre, Sydney, Australia},
	series = {Proceedings of {Machine} {Learning} {Research}},
	title = {Projection-free {Distributed} {Online} {Learning} in {Networks}},
	volume = {70},
	url = {http://proceedings.mlr.press/v70/zhang17g.html},
	abstract = {The conditional gradient algorithm has regained a surge of research interest in recent years due to its high efficiency in handling large-scale machine learning problems. However, none of existing studies has explored it in the distributed online learning setting, where locally light computation is assumed. In this paper, we fill this gap by proposing the distributed online conditional gradient algorithm, which eschews the expensive projection operation needed in its counterpart algorithms by exploiting much simpler linear optimization steps. We give a regret bound for the proposed algorithm as a function of the network size and topology, which will be smaller on smaller graphs or “well-connected” graphs. Experiments on two large-scale real-world datasets for a multiclass classification task confirm the computational benefit of the proposed algorithm and also verify the theoretical regret bound.},
	booktitle = {Proceedings of the 34th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Zhang, Wenpeng and Zhao, Peilin and Zhu, Wenwu and Hoi, Steven C. H. and Zhang, Tong},
	editor = {Precup, Doina and Teh, Yee Whye},
	month = aug,
	year = {2017},
	pages = {4054--4062}
}

@inproceedings{Lane:1998:AOL:3000292.3000339,
 author = {Lane, Terran and Brodley, Carla E.},
 title = {Approaches to Online Learning and Concept Drift for User Identification in Computer Security},
 booktitle = {Proceedings of the Fourth International Conference on Knowledge Discovery and Data Mining},
 series = {KDD'98},
 year = {1998},
 location = {New York, NY},
 pages = {259--263},
 numpages = {5},
 url = {http://dl.acm.org/citation.cfm?id=3000292.3000339},
 acmid = {3000339},
 publisher = {AAAI Press},
} 

@inproceedings{gepperth_incremental_2016,
	address = {Bruges, Belgium},
	title = {Incremental learning algorithms and applications},
	url = {https://hal.archives-ouvertes.fr/hal-01418129},
	abstract = {Incremental learning refers to learning from streaming data, which arrive over time, with limited memory resources and, ideally, without sacrificing model accuracy. This setting fits different application scenarios where lifelong learning is relevant, e.g. due to changing environments , and it offers an elegant scheme for big data processing by means of its sequential treatment. In this contribution, we formalise the concept of incremental learning, we discuss particular challenges which arise in this setting, and we give an overview about popular approaches, its theoretical foundations, and applications which emerged in the last years.},
	urldate = {2018-12-23},
	booktitle = {European {Symposium} on {Artificial} {Neural} {Networks} ({ESANN})},
	author = {Gepperth, Alexander and Hammer, Barbara},
	year = {2016}
}

@incollection{bower_catastrophic_1989,
	series = {Psychology of {Learning} and {Motivation}},
	title = {Catastrophic {Interference} in {Connectionist} {Networks}: {The} {Sequential} {Learning} {Problem}},
	volume = {24},
	url = {http://www.sciencedirect.com/science/article/pii/S0079742108605368},
	abstract = {Publisher Summary Connectionist networks in which information is stored in weights on connections among simple processing units have attracted considerable interest in cognitive science. Much of the interest centers around two characteristics of these networks. First, the weights on connections between units need not be prewired by the model builder but rather may be established through training in which items to be learned are presented repeatedly to the network and the connection weights are adjusted in small increments according to a learning algorithm. Second, the networks may represent information in a distributed fashion. This chapter discusses the catastrophic interference in connectionist networks. Distributed representations established through the application of learning algorithms have several properties that are claimed to be desirable from the standpoint of modeling human cognition. These properties include content-addressable memory and so-called automatic generalization in which a network trained on a set of items responds correctly to other untrained items within the same domain. New learning may interfere catastrophically with old learning when networks are trained sequentially. The analysis of the causes of interference implies that at least some interference will occur whenever new learning may alter weights involved in representing old learning, and the simulation results demonstrate only that interference is catastrophic in some specific networks.},
	publisher = {Academic Press},
	author = {McCloskey, Michael and Cohen, Neal J.},
	editor = {Bower, Gordon H.},
	year = {1989},
	doi = {10.1016/S0079-7421(08)60536-8},
	pages = {109 -- 165}
}